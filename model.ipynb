{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.max_colwidth = 1000\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 59 s\n"
     ]
    }
   ],
   "source": [
    "train_orig = pd.read_csv(\"/home/sroberts/train.csv\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:41.987077+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:42.870083+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  target                                       comment_text  severe_toxicity  obscene  identity_attack  insult  threat  asian  atheist  bisexual  black  buddhist  christian  female  heterosexual  hindu  homosexual_gay_or_lesbian  intellectual_or_learning_disability  jewish  latino  male  muslim  other_disability  other_gender  other_race_or_ethnicity  other_religion  other_sexual_orientation  physical_disability  psychiatric_or_mental_illness  transgender  white                   created_date  publication_id  parent_id  article_id    rating  funny  wow  sad  likes  disagree  sexual_explicit  identity_annotator_count  toxicity_annotator_count\n",
       "0  59848     0.0  This is so cool. It's like, 'would you want yo...              0.0      0.0              0.0     0.0     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2015-09-29 10:50:41.987077+00               2        NaN        2006  rejected      0    0    0      0         0              0.0                         0                         4\n",
       "1  59849     0.0  Thank you!! This would make my life a lot less...              0.0      0.0              0.0     0.0     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2015-09-29 10:50:42.870083+00               2        NaN        2006  rejected      0    0    0      0         0              0.0                         0                         4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 38.9 ms\n"
     ]
    }
   ],
   "source": [
    "train_orig[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-29 10:50:48.488476+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59859</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>ur a sh*tty comment.</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:50.865549+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target                          comment_text  severe_toxicity   obscene  identity_attack    insult  threat  asian  atheist  bisexual  black  buddhist  christian  female  heterosexual  hindu  homosexual_gay_or_lesbian  intellectual_or_learning_disability  jewish  latino  male  muslim  other_disability  other_gender  other_race_or_ethnicity  other_religion  other_sexual_orientation  physical_disability  psychiatric_or_mental_illness  transgender  white                   created_date  publication_id  parent_id  article_id    rating  funny  wow  sad  likes  disagree  sexual_explicit  identity_annotator_count  toxicity_annotator_count\n",
       "4  59856  0.893617  haha you guys are a bunch of losers.         0.021277  0.000000         0.021277  0.872340     0.0    0.0      0.0       0.0    0.0       0.0        0.0     0.0           0.0    0.0                        0.0                                 0.25     0.0     0.0   0.0     0.0               0.0           0.0                      0.0             0.0                       0.0                  0.0                            0.0          0.0    0.0  2015-09-29 10:50:48.488476+00               2        NaN        2006  rejected      0    0    0      1         0         0.000000                         4                        47\n",
       "5  59859  0.666667                  ur a sh*tty comment.         0.047619  0.638095         0.000000  0.333333     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2015-09-29 10:50:50.865549+00               2        NaN        2006  rejected      0    0    0      0         0         0.009524                         0                       105"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 112 ms\n"
     ]
    }
   ],
   "source": [
    "# toxic comments\n",
    "train_orig[train_orig.target >= 0.5][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NaNs with Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 846 ms\n"
     ]
    }
   ],
   "source": [
    "train_orig.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### describe/summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_df = train_orig.copy()\n",
    "desc_df.loc[desc_df.target >= 0.5, \"toxic\"] = True\n",
    "desc_df.toxic.fillna(False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_col = list(set(desc_df.columns.tolist()) - set(['target','id']))\n",
    "desc_df[rel_col].groupby(\"toxic\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lower case, stopwords, puncutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('all')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.3 ms\n"
     ]
    }
   ],
   "source": [
    "english_stopwords = set(stopwords.words('english'))\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.01 ms\n"
     ]
    }
   ],
   "source": [
    "def clean_message(msg):\n",
    "    msg = msg.lower()\n",
    "    msg_tokens = nltk.word_tokenize(msg)\n",
    "    clean_msg_tokens = [w for w in msg_tokens if w not in english_stopwords]\n",
    "    clean_msg_tokens_puct = [w for w in clean_msg_tokens if w not in string.punctuation]\n",
    "    lemmatized_token = [lemmatizer.lemmatize(w) for w in clean_msg_tokens_puct]\n",
    "    return lemmatized_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4min 35s\n"
     ]
    }
   ],
   "source": [
    "train_orig[\"clean\"] = train_orig.comment_text.apply(lambda x: \" \".join([words.strip(string.punctuation).lower()\n",
    "for words in [word for word in x.split() if (word not in string.punctuation) & (word not in english_stopwords)]]))\n",
    "train_orig[\"clean\"] = train_orig[\"clean\"].apply(lambda x: \" \".join([lemmatizer.lemmatize(w) \n",
    "                                                                    for w in w_tokenizer.tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          this cool it's like would want mother read this really great idea well done\n",
       "1    thank you this would make life lot le anxiety-inducing keep up let anyone get way\n",
       "Name: clean, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.34 ms\n"
     ]
    }
   ],
   "source": [
    "train_orig['clean'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "voc_counter = Counter(\" \".join(train_orig.clean).split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 139 ms\n"
     ]
    }
   ],
   "source": [
    "voc_set = dict((k, v) for k, v in voc_counter.items() if v > 10)\n",
    "voc_dict = {w:i for w,i in zip(voc_set, range(len(voc_set)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "train_orig[\"clean\"] = train_orig[\"clean\"].apply(lambda x: x.split())\n",
    "train_orig.loc[train_orig.target >= 0.5, \"toxic\"] = True\n",
    "train_orig.toxic.fillna(False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(train_orig), len(voc_set)), dtype = np.int8 )\n",
    "for msg_idx, msg in enumerate(train_orig['clean']):\n",
    "    for word in set(msg):\n",
    "        if word in voc_dict:\n",
    "            X[msg_idx][voc_dict[word]] = msg.count(word)\n",
    "            \n",
    "y = np.where(train_orig.toxic == True, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply same methods to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_orig = pd.read_csv(\"/home/sroberts/test.csv\", engine='python')\n",
    "test_orig.fillna(0, inplace=True)\n",
    "test_orig['clean'] = test_orig.comment_text.apply(lambda x: \" \".join([words.strip(string.punctuation).lower()\n",
    "for words in [word for word in x.split() if (word not in string.punctuation) & (word not in english_stopwords)]]))\n",
    "test_orig[\"clean\"] = test_orig[\"clean\"].apply(lambda x: \" \".join([lemmatizer.lemmatize(w) \n",
    "                                                                    for w in w_tokenizer.tokenize(x)]))\n",
    "test_orig[\"clean\"] = test_orig[\"clean\"].apply(lambda x: x.split())\n",
    "\n",
    "X_final = np.zeros((len(test_orig), len(voc_set)), dtype = np.int8 )\n",
    "for msg_idx, msg in enumerate(test_orig['clean']):\n",
    "    for word in set(msg):\n",
    "        if word in voc_dict:\n",
    "            X_final[msg_idx][voc_dict[word]] = msg.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "train_X,test_X,train_y,test_y = train_test_split(X,y,test_size=.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 958 µs\n"
     ]
    }
   ],
   "source": [
    "lr = SGDClassifier(loss=\"log\", n_jobs=-1, class_weight={0: 0.1,1:0.9})\n",
    "\n",
    "#classes = np.unique([\"toxic\", \"not_toxic\"])\n",
    "classes = np.unique([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create batches of data for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.53 ms\n"
     ]
    }
   ],
   "source": [
    "minibatches = [(train_X[:100000,:],train_y[:100000]),\n",
    "               (train_X[100001:200000,:],train_y[100001:200000]),\n",
    "               (train_X[200001:300000,:],train_y[200001:300000]),\n",
    "               (train_X[300001:400000,:],train_y[300001:400000]),\n",
    "               (train_X[400001:500000,:],train_y[400001:500000]),\n",
    "               (train_X[500001:600000,:],train_y[500001:600000]),\n",
    "               (train_X[600001:700000,:],train_y[600001:700000]),\n",
    "               (train_X[700001:800000,:],train_y[700001:800000]),\n",
    "               (train_X[800001:900000,:],train_y[800001:900000]),\n",
    "               (train_X[900001:1000000,:],train_y[900001:1000000]),\n",
    "               (train_X[1000001:1100000,:],train_y[1000001:1100000]),\n",
    "               (train_X[1100001:1200000,:],train_y[1100001:1200000]),\n",
    "               (train_X[1200001:1300000,:],train_y[1200001:1300000]),\n",
    "               (train_X[1300001:1400000,:],train_y[1300001:1400000]),\n",
    "               (train_X[1400001:,:],train_y[1400001:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15min 9s\n"
     ]
    }
   ],
   "source": [
    "for xs, ys in minibatches:\n",
    "    lr.partial_fit(xs, ys, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SGDC.pkl', 'wb') as fid:\n",
    "    pickle.dump(lr, fid)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SGDC.pkl', 'rb') as fid:\n",
    "    lr = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how model did on our test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9188918900200845"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "print(lr.score(test_X, test_y))\n",
    "print(lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look deeper at our model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min\n"
     ]
    }
   ],
   "source": [
    "pred_y = lr.predict(test_X)\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "print(precision_score(test_y, pred_y))\n",
    "print(recall_score(test_y, pred_y))\n",
    "print(f1_score(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdc_test = lr.predict(X_final)\n",
    "with open('sgdc_test.pkl', 'wb') as fid:\n",
    "    pickle.dump(sgdc_test, fid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9335521850543667\n",
      "[[320218  11972]\n",
      " [ 12014  16771]]\n",
      "0.5830552078987623\n",
      "time: 21min\n"
     ]
    }
   ],
   "source": [
    "lr = SGDClassifier(loss=\"log\", n_jobs=-1, class_weight={0: 0.15,1:0.85})\n",
    "\n",
    "for xs, ys in minibatches:\n",
    "    lr.partial_fit(xs, ys, classes=classes)\n",
    "    \n",
    "with open('SGDC_classwieghts_15_85.pkl', 'wb') as fid:\n",
    "    pickle.dump(lr, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SGDC_classwieghts_15_85.pkl', 'rb') as fid:\n",
    "    lr = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.score(test_X, test_y))\n",
    "print(lr.intercept_)\n",
    "pred_y = lr.predict(test_X)\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "print(precision_score(test_y, pred_y))\n",
    "print(recall_score(test_y, pred_y))\n",
    "print(f1_score(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdc_test = lr.predict(X_final)\n",
    "with open('sgdc_weights_test.pkl', 'wb') as fid:\n",
    "    pickle.dump(sgdc_test, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 862 µs\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "classes = np.unique([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 58min 44s\n"
     ]
    }
   ],
   "source": [
    "for xs, ys in minibatches:\n",
    "    clf.partial_fit(xs, ys, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mlpclassifier1.pkl', 'wb') as fid:\n",
    "    pickle.dump(clf, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mlpclassifier1.pkl', 'rb') as fid:\n",
    "    clf = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07974236442966964"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(test_X, test_y))\n",
    "print(clf.coefs_)\n",
    "print(clf.intercepts_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = clf.predict(test_X)\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "print(precision_score(test_y, pred_y))\n",
    "print(recall_score(test_y, pred_y))\n",
    "print(f1_score(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_test = clf.predict(X_final)\n",
    "with open('mlp_test.pkl', 'wb') as fid:\n",
    "    pickle.dump(mlp_test, fid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_test = clf.predict(X_final)\n",
    "with open('mlp_weights_test.pkl', 'wb') as fid:\n",
    "    pickle.dump(mlp_test, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.47 ms\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-a2a5c25a9d57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/toxic/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, classes, sample_weight)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \"\"\"\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/toxic/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/envs/toxic/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15min 38s\n"
     ]
    }
   ],
   "source": [
    "for xs, ys in minibatches:\n",
    "    nb.partial_fit(xs, ys, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('naivebayes.pkl', 'wb') as fid:\n",
    "    pickle.dump(nb, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('naivebayes.pkl', 'rb') as fid:\n",
    "    nb = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nb.score(test_X, test_y))\n",
    "print(nb.intercept_)\n",
    "print(nb.coef_)\n",
    "print(nb.feature_count_)\n",
    "print(nb.class_count_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = nb.predict(test_X)\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "print(precision_score(test_y, pred_y))\n",
    "print(recall_score(test_y, pred_y))\n",
    "print(f1_score(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test = nb.predict(X_final)\n",
    "with open('nb_test.pkl', 'wb') as fid:\n",
    "    pickle.dump(nb_test, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45791813837523376\n",
      "[[138623 193567]\n",
      " [  2111  26674]]\n",
      "0.2142266269385526\n",
      "time: 19min 33s\n"
     ]
    }
   ],
   "source": [
    "nb= MultinomialNB(alpha= 0.1, fit_prior = True, class_prior = [0.15, 0.85])\n",
    "for xs, ys in minibatches:\n",
    "    nb.partial_fit(xs, ys, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('naivebayes_weights_class_weights.pkl', 'wb') as fid:\n",
    "    pickle.dump(nb, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('naivebayes_weights_class_weights.pkl', 'rb') as fid:\n",
    "    nb = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nb.score(test_X, test_y))\n",
    "print(nb.intercept_)\n",
    "print(nb.coef_)\n",
    "print(nb.feature_count_)\n",
    "print(nb.class_count_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = nb.predict(test_X)\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "print(precision_score(test_y, pred_y))\n",
    "print(recall_score(test_y, pred_y))\n",
    "print(f1_score(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test = nb.predict(X_final)\n",
    "with open('nb_weights_test.pkl', 'wb') as fid:\n",
    "    pickle.dump(nb_test, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(warm_start=True, n_estimators=100, max_depth=8, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xs, ys in minibatches:\n",
    "    rf.fit(xs,ys)\n",
    "    rf.n_estimators += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('randomforest.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('randomforest.pkl', 'rb') as fid:\n",
    "    rf = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf.score(test_X, test_y))\n",
    "print(rf.estimators_)\n",
    "print(rf.n_features_)\n",
    "print(rf.n_outputs_)\n",
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = rf.predict(test_X)\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "print(precision_score(test_y, pred_y))\n",
    "print(recall_score(test_y, pred_y))\n",
    "print(f1_score(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test = rf.predict(X_final)\n",
    "with open('rf_test.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf_test, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(warm_start=True, n_estimators=100, max_depth=8, n_jobs=-1, class_weight={0: 0.15,1:0.85})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xs, ys in minibatches:\n",
    "    rf.fit(xs,ys)\n",
    "    rf.n_estimators += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('randomforest_classweights.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('randomforest_classweights.pkl', 'rb') as fid:\n",
    "    rf = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf.score(test_X, test_y))\n",
    "print(rf.estimators_)\n",
    "print(rf.n_features_)\n",
    "print(rf.n_outputs_)\n",
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = rf.predict(test_X)\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "print(precision_score(test_y, pred_y))\n",
    "print(recall_score(test_y, pred_y))\n",
    "print(f1_score(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test = rf.predict(X_final)\n",
    "with open('rf_weights_test.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf_test, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
