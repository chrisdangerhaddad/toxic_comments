{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 59 s\n"
     ]
    }
   ],
   "source": [
    "train_orig = pd.read_csv(\"/home/sroberts/train.csv\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:41.987077+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:42.870083+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  target                                       comment_text  severe_toxicity  obscene  identity_attack  insult  threat  asian  atheist  bisexual  black  buddhist  christian  female  heterosexual  hindu  homosexual_gay_or_lesbian  intellectual_or_learning_disability  jewish  latino  male  muslim  other_disability  other_gender  other_race_or_ethnicity  other_religion  other_sexual_orientation  physical_disability  psychiatric_or_mental_illness  transgender  white                   created_date  publication_id  parent_id  article_id    rating  funny  wow  sad  likes  disagree  sexual_explicit  identity_annotator_count  toxicity_annotator_count\n",
       "0  59848     0.0  This is so cool. It's like, 'would you want yo...              0.0      0.0              0.0     0.0     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2015-09-29 10:50:41.987077+00               2        NaN        2006  rejected      0    0    0      0         0              0.0                         0                         4\n",
       "1  59849     0.0  Thank you!! This would make my life a lot less...              0.0      0.0              0.0     0.0     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2015-09-29 10:50:42.870083+00               2        NaN        2006  rejected      0    0    0      0         0              0.0                         0                         4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 38.9 ms\n"
     ]
    }
   ],
   "source": [
    "train_orig[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-29 10:50:48.488476+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59859</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>ur a sh*tty comment.</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:50.865549+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target                          comment_text  severe_toxicity   obscene  identity_attack    insult  threat  asian  atheist  bisexual  black  buddhist  christian  female  heterosexual  hindu  homosexual_gay_or_lesbian  intellectual_or_learning_disability  jewish  latino  male  muslim  other_disability  other_gender  other_race_or_ethnicity  other_religion  other_sexual_orientation  physical_disability  psychiatric_or_mental_illness  transgender  white                   created_date  publication_id  parent_id  article_id    rating  funny  wow  sad  likes  disagree  sexual_explicit  identity_annotator_count  toxicity_annotator_count\n",
       "4  59856  0.893617  haha you guys are a bunch of losers.         0.021277  0.000000         0.021277  0.872340     0.0    0.0      0.0       0.0    0.0       0.0        0.0     0.0           0.0    0.0                        0.0                                 0.25     0.0     0.0   0.0     0.0               0.0           0.0                      0.0             0.0                       0.0                  0.0                            0.0          0.0    0.0  2015-09-29 10:50:48.488476+00               2        NaN        2006  rejected      0    0    0      1         0         0.000000                         4                        47\n",
       "5  59859  0.666667                  ur a sh*tty comment.         0.047619  0.638095         0.000000  0.333333     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2015-09-29 10:50:50.865549+00               2        NaN        2006  rejected      0    0    0      0         0         0.009524                         0                       105"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 112 ms\n"
     ]
    }
   ],
   "source": [
    "# toxic comments\n",
    "train_orig[train_orig.target >= 0.5][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill NaNs with Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 846 ms\n"
     ]
    }
   ],
   "source": [
    "train_orig.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# describe/summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_df = train_orig.copy()\n",
    "desc_df.loc[desc_df.target >= 0.5, \"toxic\"] = True\n",
    "desc_df.toxic.fillna(False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_col = list(set(desc_df.columns.tolist()) - set(['target','id']))\n",
    "desc_df[rel_col].groupby(\"toxic\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lower case, stopwords, puncutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('all')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 643 ms\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 1000\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.3 ms\n"
     ]
    }
   ],
   "source": [
    "english_stopwords = set(stopwords.words('english'))\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.01 ms\n"
     ]
    }
   ],
   "source": [
    "def clean_message(msg):\n",
    "    msg = msg.lower()\n",
    "    msg_tokens = nltk.word_tokenize(msg)\n",
    "    clean_msg_tokens = [w for w in msg_tokens if w not in english_stopwords]\n",
    "    clean_msg_tokens_puct = [w for w in clean_msg_tokens if w not in string.punctuation]\n",
    "    lemmatized_token = [lemmatizer.lemmatize(w) for w in clean_msg_tokens_puct]\n",
    "    return lemmatized_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poorn\n",
    "#train_orig['clean'] = train_orig.comment_text.apply(clean_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4min 35s\n"
     ]
    }
   ],
   "source": [
    "train_orig[\"clean\"] = train_orig.comment_text.apply(lambda x: \" \".join([words.strip(string.punctuation).lower()\n",
    "for words in [word for word in x.split() if (word not in string.punctuation) & (word not in english_stopwords)]]))\n",
    "train_orig[\"clean\"] = train_orig[\"clean\"].apply(lambda x: \" \".join([lemmatizer.lemmatize(w) \n",
    "                                                                    for w in w_tokenizer.tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          this cool it's like would want mother read this really great idea well done\n",
       "1    thank you this would make life lot le anxiety-inducing keep up let anyone get way\n",
       "Name: clean, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.34 ms\n"
     ]
    }
   ],
   "source": [
    "train_orig['clean'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.27 ms\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooran\n",
    "#voc_counter = Counter(messages_df.clean_message.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "voc_counter = Counter(\" \".join(train_orig.clean).split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 262 µs\n"
     ]
    }
   ],
   "source": [
    "#for w,c in voc_counter.items():\n",
    "#    print(w,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 139 ms\n"
     ]
    }
   ],
   "source": [
    "voc_set = dict((k, v) for k, v in voc_counter.items() if v > 10)\n",
    "voc_dict = {w:i for w,i in zip(voc_set, range(len(voc_set)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "train_orig[\"clean\"] = train_orig[\"clean\"].apply(lambda x: x.split())\n",
    "train_orig.loc[train_orig.target >= 0.5, \"toxic\"] = True\n",
    "train_orig.toxic.fillna(False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_orig = train_orig.sample(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(train_orig), len(voc_set)), dtype = np.int8 )\n",
    "for msg_idx, msg in enumerate(train_orig['clean']):\n",
    "    for word in set(msg):\n",
    "        if word in voc_dict:\n",
    "            X[msg_idx][voc_dict[word]] = msg.count(word)\n",
    "            \n",
    "y = np.where(train_orig.toxic == True, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "#ros = RandomOverSampler(random_state=0)\n",
    "#X_resampled, y_resampled = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('y.txt', y, fmt='%d')\n",
    "# np.savetxt('X.txt', X, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!! restart kernel !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# X = np.loadtxt('X.txt', dtype=int)\n",
    "# y = np.loadtxt('y.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 567 µs\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.88 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_size=.8\n",
    "#train_indicies = int(X.shape[0]*train_size)\n",
    "\n",
    "#np.random.shuffle(X)\n",
    "#train_X,test_X = X[:train_indicies,:],X[train_indicies:,:]\n",
    "#train_y,test_y = y[:train_indicies],  y[train_indicies:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "train_X,test_X,train_y,test_y = train_test_split(X,y,test_size=.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sroberts/anaconda3/envs/toxic/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "#ros = RandomOverSampler(random_state=0)\n",
    "#X_resampled, y_resampled = ros.fit_resample(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 958 µs\n"
     ]
    }
   ],
   "source": [
    "lr = SGDClassifier(loss=\"log\", n_jobs=-1, class_weight={0: 0.1,1:0.9})\n",
    "\n",
    "#classes = np.unique([\"toxic\", \"not_toxic\"])\n",
    "classes = np.unique([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.53 ms\n"
     ]
    }
   ],
   "source": [
    "minibatches = [(train_X[:100000,:],train_y[:100000]),\n",
    "               (train_X[100001:200000,:],train_y[100001:200000]),\n",
    "               (train_X[200001:300000,:],train_y[200001:300000]),\n",
    "               (train_X[300001:400000,:],train_y[300001:400000]),\n",
    "               (train_X[400001:500000,:],train_y[400001:500000]),\n",
    "               (train_X[500001:600000,:],train_y[500001:600000]),\n",
    "               (train_X[600001:700000,:],train_y[600001:700000]),\n",
    "               (train_X[700001:800000,:],train_y[700001:800000]),\n",
    "               (train_X[800001:900000,:],train_y[800001:900000]),\n",
    "               (train_X[900001:1000000,:],train_y[900001:1000000]),\n",
    "               (train_X[1000001:1100000,:],train_y[1000001:1100000]),\n",
    "               (train_X[1100001:1200000,:],train_y[1100001:1200000]),\n",
    "               (train_X[1200001:1300000,:],train_y[1200001:1300000]),\n",
    "               (train_X[1300001:1400000,:],train_y[1300001:1400000]),\n",
    "               (train_X[1400001:,:],train_y[1400001:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.32 ms\n"
     ]
    }
   ],
   "source": [
    "# minibatches = [(X_resampled[:100000,:],y_resampled[:100000]),\n",
    "#                (X_resampled[100001:200000,:],y_resampled[100001:200000]),\n",
    "#                (X_resampled[200001:300000,:],y_resampled[200001:300000]),\n",
    "#                (X_resampled[300001:400000,:],y_resampled[300001:400000]),\n",
    "#                (X_resampled[400001:500000,:],y_resampled[400001:500000]),\n",
    "#                (X_resampled[500001:600000,:],y_resampled[500001:600000]),\n",
    "#                (X_resampled[600001:700000,:],y_resampled[600001:700000]),\n",
    "#                (X_resampled[700001:800000,:],y_resampled[700001:800000]),\n",
    "#                (X_resampled[800001:900000,:],y_resampled[800001:900000]),\n",
    "#                (X_resampled[900001:1000000,:],y_resampled[900001:1000000]),\n",
    "#                (X_resampled[1000001:1100000,:],y_resampled[1000001:1100000]),\n",
    "#                (X_resampled[1100001:1200000,:],y_resampled[1100001:1200000]),\n",
    "#                (X_resampled[1200001:1300000,:],y_resampled[1200001:1300000]),\n",
    "#                (X_resampled[1300001:1400000,:],y_resampled[1300001:1400000]),\n",
    "#                (X_resampled[1400001:,:],y_resampled[1400001:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15min 9s\n"
     ]
    }
   ],
   "source": [
    "for xs, ys in minibatches:\n",
    "    lr.partial_fit(xs, ys, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9188918900200845"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "lr.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.77380937])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.19 ms\n"
     ]
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.loss_function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 538 µs\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min\n"
     ]
    }
   ],
   "source": [
    "pred_y = lr.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[311869,  20321],\n",
       "       [  8957,  19828]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 323 ms\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 428 µs\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class weights (0:0.1, 1:0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5752749006295877"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "f1_score(test_y, pred_y)\n",
    "#do we need to specify something for weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-weighted SGDC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "pred_y1 = lr_loaded.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[329735,   2455],\n",
       "       [ 19524,   9261]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 309 ms\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix(test_y, pred_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45732204143107574"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 97.2 ms\n"
     ]
    }
   ],
   "source": [
    "f1_score(test_y, pred_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with class weightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9394113165731699\n",
      "[[324971   7219]\n",
      " [ 14652  14133]]\n",
      "0.5637752557991105\n",
      "time: 21min 27s\n"
     ]
    }
   ],
   "source": [
    "lr = SGDClassifier(loss=\"log\", n_jobs=-1, class_weight={0: 0.2,1:0.8})\n",
    "\n",
    "for xs, ys in minibatches:\n",
    "    lr.partial_fit(xs, ys, classes=classes)\n",
    "\t\n",
    "print(lr.score(test_X, test_y))\n",
    "\n",
    "pred_y = lr.predict(test_X)\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "print(f1_score(test_y, pred_y))\n",
    "\n",
    "with open('SGDC_classwieghts_2_8.pkl', 'wb') as fid:\n",
    "    pickle.dump(lr, fid)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9335521850543667\n",
      "[[320218  11972]\n",
      " [ 12014  16771]]\n",
      "0.5830552078987623\n",
      "time: 21min\n"
     ]
    }
   ],
   "source": [
    "lr = SGDClassifier(loss=\"log\", n_jobs=-1, class_weight={0: 0.15,1:0.85})\n",
    "\n",
    "for xs, ys in minibatches:\n",
    "    lr.partial_fit(xs, ys, classes=classes)\n",
    "\t\n",
    "print(lr.score(test_X, test_y))\n",
    "\n",
    "pred_y = lr.predict(test_X)\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "print(f1_score(test_y, pred_y))\n",
    "\n",
    "with open('SGDC_classwieghts_15_85.pkl', 'wb') as fid:\n",
    "    pickle.dump(lr, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5834811954214939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5826298419315615"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 206 ms\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(test_y, pred_y))\n",
    "recall_score(test_y, pred_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9004640210540896\n",
      "[[303291  28899]\n",
      " [  7031  21754]]\n",
      "0.5476975754676604\n",
      "time: 21min 24s\n"
     ]
    }
   ],
   "source": [
    "lr = SGDClassifier(loss=\"log\", n_jobs=-1, class_weight={0: 0.08691991761,1:0.91308008239})\n",
    "\n",
    "for xs, ys in minibatches:\n",
    "    lr.partial_fit(xs, ys, classes=classes)\n",
    "\t\n",
    "print(lr.score(test_X, test_y))\n",
    "\n",
    "pred_y = lr.predict(test_X)\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "print(f1_score(test_y, pred_y))\n",
    "\n",
    "with open('SGDC_classwieghts_08_92.pkl', 'wb') as fid:\n",
    "    pickle.dump(lr, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91308008239"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.98 ms\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6 0.4\n",
      "0.7 0.30000000000000004\n",
      "0.8 0.19999999999999996\n",
      "0.9 0.09999999999999998\n",
      "time: 1.04 ms\n"
     ]
    }
   ],
   "source": [
    "weight_score_dict = {}\n",
    "for i in range(6,10):\n",
    "    i = i/10\n",
    "    j = 1- i\n",
    "    print(i,j)\n",
    "    #lr = SGDClassifier(loss=\"log\", n_jobs=-1, class_weight={0:j,1:i})\n",
    "    #for xs, ys in minibatches:\n",
    "        #lr.partial_fit(xs, ys, classes=classes)\n",
    "    #weight_score_dict[(j,i)] = lr.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.77 ms\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# save the classifier\n",
    "with open('SGDC_classwieghts.pkl', 'wb') as fid:\n",
    "    pickle.dump(lr, fid)    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.57 ms\n"
     ]
    }
   ],
   "source": [
    "# load it again\n",
    "with open('my_dumped_classifier.pkl', 'rb') as fid:\n",
    "    lr_loaded = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_loaded.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, still works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_orig = pd.read_csv(\"/home/sroberts/test.csv\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_orig[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_orig.fillna(0, inplace=True)\n",
    "test_orig['clean'] = test_orig.comment_text.apply(lambda x: \" \".join([words.strip(string.punctuation).lower()\n",
    "for words in [word for word in x.split() if (word not in string.punctuation) & (word not in english_stopwords)]]))\n",
    "test_orig[\"clean\"] = test_orig[\"clean\"].apply(lambda x: \" \".join([lemmatizer.lemmatize(w) \n",
    "                                                                    for w in w_tokenizer.tokenize(x)]))\n",
    "#voc_counter = Counter(\" \".join(test_orig.clean).split(\" \"))\n",
    "#voc_set = dict((k, v) for k, v in voc_counter.items() if v > 10)\n",
    "#voc_dict = {w:i for w,i in zip(voc_set, range(len(voc_set)))}\n",
    "test_orig[\"clean\"] = test_orig[\"clean\"].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_orig.loc[test_orig.target >= 0.5, \"toxic\"] = True\n",
    "#test_orig.toxic.fillna(False,inplace=True)\n",
    "X = np.zeros((len(test_orig), len(voc_set)), dtype = np.int8 )\n",
    "for msg_idx, msg in enumerate(test_orig['clean']):\n",
    "    for word in set(msg):\n",
    "        if word in voc_dict:\n",
    "            X[msg_idx][voc_dict[word]] = msg.count(word)\n",
    "            \n",
    "#y = np.where(test_orig.toxic == True, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.79 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X,test_X,train_y,test_y = train_test_split(X,y,test_size=.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 862 µs\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "#classes = np.unique([\"toxic\", \"not_toxic\"])\n",
    "classes = np.unique([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatches = [(train_X[:100000,:],train_y[:100000]),\n",
    "#                (train_X[100001:200000,:],train_y[100001:200000]),\n",
    "#                (train_X[200001:300000,:],train_y[200001:300000]),\n",
    "#                (train_X[300001:400000,:],train_y[300001:400000]),\n",
    "#                (train_X[400001:500000,:],train_y[400001:500000]),\n",
    "#                (train_X[500001:600000,:],train_y[500001:600000]),\n",
    "#                (train_X[600001:700000,:],train_y[600001:700000]),\n",
    "#                (train_X[700001:800000,:],train_y[700001:800000]),\n",
    "#                (train_X[800001:900000,:],train_y[800001:900000]),\n",
    "#                (train_X[900001:1000000,:],train_y[900001:1000000]),\n",
    "#                (train_X[1000001:1100000,:],train_y[1000001:1100000]),\n",
    "#                (train_X[1100001:1200000,:],train_y[1100001:1200000]),\n",
    "#                (train_X[1200001:1300000,:],train_y[1200001:1300000]),\n",
    "#                (train_X[1300001:1400000,:],train_y[1300001:1400000]),\n",
    "#                (train_X[1400001:,:],train_y[1400001:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 58min 44s\n"
     ]
    }
   ],
   "source": [
    "for xs, ys in minibatches:\n",
    "    clf.partial_fit(xs, ys, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07974236442966964"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "clf.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.01401458,  0.00463394,  0.00501281,  0.01094861, -0.00292222],\n",
       "        [-0.0082903 , -0.00609479, -0.00306301, -0.00233737,  0.00090535],\n",
       "        [ 0.00309259,  0.00380837, -0.00150815,  0.00792303, -0.00715011],\n",
       "        ...,\n",
       "        [ 0.00489398, -0.00787083, -0.00903126, -0.00810709,  0.00217807],\n",
       "        [-0.00248438, -0.00478913,  0.00449138, -0.00571414, -0.00093135],\n",
       "        [-0.00548875, -0.00639404,  0.00365674,  0.00561295,  0.00538747]]),\n",
       " array([[-0.858365  ,  0.24408481],\n",
       "        [ 0.01992537, -0.8862989 ],\n",
       "        [-0.55352744, -0.34125225],\n",
       "        [-0.70994201, -0.40900862],\n",
       "        [ 0.11107635, -0.84903885]]),\n",
       " array([[-1.00336856],\n",
       "        [ 0.72206879]])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.14 ms\n"
     ]
    }
   ],
   "source": [
    "clf.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.12492733, 0.01214916, 0.08904607, 0.10782317, 0.03537445]),\n",
       " array([ 0.3985795 , -0.06732792]),\n",
       " array([3.27367642])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.33 ms\n"
     ]
    }
   ],
   "source": [
    "clf.intercepts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.3 ms\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('mlpclassifier1_imblearn.pkl', 'wb') as fid:\n",
    "    pickle.dump(clf, fid)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mlpclassifier1_imblearn.pkl', 'rb') as fid:\n",
    "    mlp_loaded = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.7 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.47 ms\n"
     ]
    }
   ],
   "source": [
    "#nb = MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the class imbalance\n",
    "nb= MultinomialNB(alpha= 0.1, fit_prior = True, class_prior = [0.08691991761, 0.91308008239])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-a2a5c25a9d57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/toxic/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, classes, sample_weight)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \"\"\"\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/toxic/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/envs/toxic/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15min 38s\n"
     ]
    }
   ],
   "source": [
    "for xs, ys in minibatches:\n",
    "    nb.partial_fit(xs, ys, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.class_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('naivebayes_weights_class_weights.pkl', 'wb') as fid:\n",
    "    pickle.dump(nb, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('naivebayes_weights.pkl', 'rb') as fid:\n",
    "    nb_loaded = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45791813837523376\n",
      "[[138623 193567]\n",
      " [  2111  26674]]\n",
      "0.2142266269385526\n",
      "time: 19min 33s\n"
     ]
    }
   ],
   "source": [
    "nb= MultinomialNB(alpha= 0.1, fit_prior = True, class_prior = [0.08691991761, 0.91308008239])\n",
    "for xs, ys in minibatches:\n",
    "    nb.partial_fit(xs, ys, classes=classes)\n",
    "\n",
    "print(nb.score(test_X, test_y))\n",
    "\n",
    "pred_y = nb.predict(test_X)\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "print(f1_score(test_y, pred_y))\n",
    "\n",
    "with open('naivebayes_weights_class_weights.pkl', 'wb') as fid:\n",
    "    pickle.dump(nb, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47.5 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(warm_start=True, n_estimators=1, n_jobs=-1, class_weight={0: 0.1,1:0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xs, ys in minibatches:\n",
    "    rf.fit(xs,ys)\n",
    "    rf.n_estimators += 1\n",
    "    #nb.partial_fit(xs, ys, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.n_outputs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('randomforest_classweights.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('randomforest.pkl', 'rb') as fid:\n",
    "    rf_loaded = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with RF parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier(warm_start=True, n_estimators=200, max_depth=8, n_jobs=-1,class_weight={0: 0.1,1:0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xs, ys in minibatches:\n",
    "    rf2.fit(xs,ys)\n",
    "    rf2.n_estimators += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf2.estimators_)\n",
    "print(rf2.n_features_)\n",
    "print(rf2.n_outputs_)\n",
    "print(rf2.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('randomforest2_classweights.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf2, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('randomforest2.pkl', 'rb') as fid:\n",
    "    rf2_loaded = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9236927765080685\n",
      "[[330447   1743]\n",
      " [ 25802   2983]]\n",
      "0.1780310942675539\n",
      "time: 12min 49s\n"
     ]
    }
   ],
   "source": [
    "rf2 = RandomForestClassifier(warm_start=True, n_estimators=200, max_depth=8, n_jobs=-1,class_weight={0: 0.08691991761,1:0.91308008239})\n",
    "for xs, ys in minibatches:\n",
    "    rf2.fit(xs,ys)\n",
    "    rf2.n_estimators += 1\n",
    "    \n",
    "print(rf2.score(test_X, test_y))\n",
    "\n",
    "pred_y = rf2.predict(test_X)\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "print(f1_score(test_y, pred_y))\n",
    "\n",
    "with open('randomforest2_classweights_08_92.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf2, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test sets after passing through models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdc_test = lr_loaded.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_test = mlp_loaded.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test = nb_loaded.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test = rf_loaded.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2_test = rf2_loaded.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sgdc_test.pkl', 'wb') as fid:\n",
    "    pickle.dump(sgdc_test, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mlp_test.pkl', 'wb') as fid:\n",
    "    pickle.dump(mlp_test, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nb_test.pkl', 'wb') as fid:\n",
    "    pickle.dump(nb_test, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rf_test.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf_test, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rf2_test.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf2_test, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig.loc[train_orig.target >= 0.5, \"toxic\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig.toxic.fillna(False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig.toxic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic",
   "language": "python",
   "name": "toxic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
